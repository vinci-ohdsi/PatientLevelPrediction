<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>R: Fits the state of the preprocessing layer to the data being...</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="R.css" />
</head><body>

<table width="100%" summary="page for adapt {keras}"><tr><td>adapt {keras}</td><td style="text-align: right;">R Documentation</td></tr></table>

<h2>Fits the state of the preprocessing layer to the data being passed</h2>

<h3>Description</h3>

<p>Fits the state of the preprocessing layer to the data being passed
</p>


<h3>Usage</h3>

<pre>
adapt(object, data, ..., batch_size = NULL, steps = NULL)
</pre>


<h3>Arguments</h3>

<table summary="R argblock">
<tr valign="top"><td><code>object</code></td>
<td>
<p>Preprocessing layer object</p>
</td></tr>
<tr valign="top"><td><code>data</code></td>
<td>
<p>The data to train on. It can be passed either as a
<code>tf.data.Dataset</code> or as an R array.</p>
</td></tr>
<tr valign="top"><td><code>...</code></td>
<td>
<p>Used for forwards and backwards compatibility. Passed on to the underlying method.</p>
</td></tr>
<tr valign="top"><td><code>batch_size</code></td>
<td>
<p>Integer or <code>NULL</code>. Number of asamples per state update. If
unspecified, <code>batch_size</code> will default to <code>32</code>. Do not specify the
batch_size if your data is in the form of datasets, generators, or
<code>keras.utils.Sequence</code> instances (since they generate batches).</p>
</td></tr>
<tr valign="top"><td><code>steps</code></td>
<td>
<p>Integer or <code>NULL</code>. Total number of steps (batches of samples)
When training with input tensors such as TensorFlow data tensors, the
default <code>NULL</code> is equal to the number of samples in your dataset divided by
the batch size, or <code>1</code> if that cannot be determined. If x is a
<code>tf.data.Dataset</code>, and <code>steps</code> is <code>NULL</code>, the epoch will run until the
input dataset is exhausted. When passing an infinitely repeating dataset,
you must specify the steps argument. This argument is not supported with
array inputs.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>After calling <code>adapt</code> on a layer, a preprocessing layer's state will not
update during training. In order to make preprocessing layers efficient in
any distribution context, they are kept constant with respect to any
compiled <code>tf.Graph</code>s that call the layer. This does not affect the layer use
when adapting each layer only once, but if you adapt a layer multiple times
you will need to take care to re-compile any compiled functions as follows:
</p>

<ul>
<li><p> If you are adding a preprocessing layer to a <code>keras.Model</code>, you need to
call <code>compile(model)</code> after each subsequent call to <code>adapt()</code>.
</p>
</li>
<li><p> If you are calling a preprocessing layer inside <code>tfdatasets::dataset_map()</code>,
you should call <code>dataset_map()</code> again on the input <code>tf.data.Dataset</code> after each
<code>adapt()</code>.
</p>
</li>
<li><p> If you are using a <code>tensorflow::tf_function()</code> directly which calls a preprocessing
layer, you need to call <code>tf_function</code> again on your callable after
each subsequent call to <code>adapt()</code>.
</p>
</li></ul>

<p><code>keras_model</code> example with multiple adapts:<div class="sourceCode r"></p>
<pre>layer &lt;- layer_normalization(axis=NULL)
adapt(layer, c(0, 2))
model &lt;- keras_model_sequential(layer)
predict(model, c(0, 1, 2)) # [1] -1  0  1

adapt(layer, c(-1, 1))
compile(model)  # This is needed to re-compile model.predict!
predict(model, c(0, 1, 2)) # [1] 0 1 2
</pre></div>
<p><code>tf.data.Dataset</code> example with multiple adapts:<div class="sourceCode r"></p>
<pre>layer &lt;- layer_normalization(axis=NULL)
adapt(layer, c(0, 2))
input_ds &lt;- tfdatasets::range_dataset(0, 3)
normalized_ds &lt;- input_ds %&gt;%
  tfdatasets::dataset_map(layer)
str(reticulate::iterate(normalized_ds))
# List of 3
#  $ :tf.Tensor([-1.], shape=(1,), dtype=float32)
#  $ :tf.Tensor([0.], shape=(1,), dtype=float32)
#  $ :tf.Tensor([1.], shape=(1,), dtype=float32)
adapt(layer, c(-1, 1))
normalized_ds &lt;- input_ds %&gt;%
  tfdatasets::dataset_map(layer) # Re-map over the input dataset.
str(reticulate::iterate(normalized_ds$as_numpy_iterator()))
# List of 3
#  $ : num [1(1d)] -1
#  $ : num [1(1d)] 0
#  $ : num [1(1d)] 1
</pre></div>


<h3>See Also</h3>


<ul>
<li> <p><a href="https://www.tensorflow.org/guide/keras/preprocessing_layers#the_adapt_method">https://www.tensorflow.org/guide/keras/preprocessing_layers#the_adapt_method</a>
</p>
</li>
<li> <p><a href="https://keras.io/guides/preprocessing_layers/#the-adapt-method">https://keras.io/guides/preprocessing_layers/#the-adapt-method</a>
</p>
</li></ul>


<hr /><div style="text-align: center;">[Package <em>keras</em> version 2.6.1 <a href="00Index.html">Index</a>]</div>
</body></html>
