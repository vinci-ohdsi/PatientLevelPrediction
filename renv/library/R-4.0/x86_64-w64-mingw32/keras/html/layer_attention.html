<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>R: Creates attention layer</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="R.css" />
</head><body>

<table width="100%" summary="page for layer_attention {keras}"><tr><td>layer_attention {keras}</td><td style="text-align: right;">R Documentation</td></tr></table>

<h2>Creates attention layer</h2>

<h3>Description</h3>

<p>Dot-product attention layer, a.k.a. Luong-style attention.
</p>


<h3>Usage</h3>

<pre>
layer_attention(
  inputs,
  use_scale = FALSE,
  causal = FALSE,
  batch_size = NULL,
  dtype = NULL,
  name = NULL,
  trainable = NULL,
  weights = NULL
)
</pre>


<h3>Arguments</h3>

<table summary="R argblock">
<tr valign="top"><td><code>inputs</code></td>
<td>
<p>a list of inputs first should be the query tensor, the second the value tensor</p>
</td></tr>
<tr valign="top"><td><code>use_scale</code></td>
<td>
<p>If True, will create a scalar variable to scale the attention scores.</p>
</td></tr>
<tr valign="top"><td><code>causal</code></td>
<td>
<p>Boolean. Set to True for decoder self-attention. Adds a mask such that position i cannot attend to positions j &gt; i.
This prevents the flow of information from the future towards the past.</p>
</td></tr>
<tr valign="top"><td><code>batch_size</code></td>
<td>
<p>Fixed batch size for layer</p>
</td></tr>
<tr valign="top"><td><code>dtype</code></td>
<td>
<p>The data type expected by the input, as a string (<code>float32</code>,
<code>float64</code>, <code>int32</code>...)</p>
</td></tr>
<tr valign="top"><td><code>name</code></td>
<td>
<p>An optional name string for the layer. Should be unique in a
model (do not reuse the same name twice). It will be autogenerated if it
isn't provided.</p>
</td></tr>
<tr valign="top"><td><code>trainable</code></td>
<td>
<p>Whether the layer weights will be updated during training.</p>
</td></tr>
<tr valign="top"><td><code>weights</code></td>
<td>
<p>Initial weights for layer.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other core layers: 
<code><a href="layer_activation.html">layer_activation</a>()</code>,
<code><a href="layer_activity_regularization.html">layer_activity_regularization</a>()</code>,
<code><a href="layer_dense_features.html">layer_dense_features</a>()</code>,
<code><a href="layer_dense.html">layer_dense</a>()</code>,
<code><a href="layer_dropout.html">layer_dropout</a>()</code>,
<code><a href="layer_flatten.html">layer_flatten</a>()</code>,
<code><a href="layer_input.html">layer_input</a>()</code>,
<code><a href="layer_lambda.html">layer_lambda</a>()</code>,
<code><a href="layer_masking.html">layer_masking</a>()</code>,
<code><a href="layer_permute.html">layer_permute</a>()</code>,
<code><a href="layer_repeat_vector.html">layer_repeat_vector</a>()</code>,
<code><a href="layer_reshape.html">layer_reshape</a>()</code>
</p>

<hr /><div style="text-align: center;">[Package <em>keras</em> version 2.6.1 <a href="00Index.html">Index</a>]</div>
</body></html>
