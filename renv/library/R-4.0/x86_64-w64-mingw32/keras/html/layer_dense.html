<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>R: Add a densely-connected NN layer to an output</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="R.css" />
</head><body>

<table width="100%" summary="page for layer_dense {keras}"><tr><td>layer_dense {keras}</td><td style="text-align: right;">R Documentation</td></tr></table>

<h2>Add a densely-connected NN layer to an output</h2>

<h3>Description</h3>

<p>Implements the operation: <code>output = activation(dot(input, kernel) + bias)</code>
where <code>activation</code> is the element-wise activation function passed as the
<code>activation</code> argument, <code>kernel</code> is a weights matrix created by the layer, and
<code>bias</code> is a bias vector created by the layer (only applicable if <code>use_bias</code>
is <code>TRUE</code>). Note: if the input to the layer has a rank greater than 2, then
it is flattened prior to the initial dot product with <code>kernel</code>.
</p>


<h3>Usage</h3>

<pre>
layer_dense(
  object,
  units,
  activation = NULL,
  use_bias = TRUE,
  kernel_initializer = "glorot_uniform",
  bias_initializer = "zeros",
  kernel_regularizer = NULL,
  bias_regularizer = NULL,
  activity_regularizer = NULL,
  kernel_constraint = NULL,
  bias_constraint = NULL,
  input_shape = NULL,
  batch_input_shape = NULL,
  batch_size = NULL,
  dtype = NULL,
  name = NULL,
  trainable = NULL,
  weights = NULL
)
</pre>


<h3>Arguments</h3>

<table summary="R argblock">
<tr valign="top"><td><code>object</code></td>
<td>
<p>What to call the new <code>Layer</code> instance with. Typically a keras
<code>Model</code>, another <code>Layer</code>, or a <code>tf.Tensor</code>/<code>KerasTensor</code>. If <code>object</code> is
missing, the <code>Layer</code> instance is returned, otherwise, <code>layer(object)</code> is
returned.</p>
</td></tr>
<tr valign="top"><td><code>units</code></td>
<td>
<p>Positive integer, dimensionality of the output space.</p>
</td></tr>
<tr valign="top"><td><code>activation</code></td>
<td>
<p>Name of activation function to use. If you don't specify
anything, no activation is applied (ie. &quot;linear&quot; activation: a(x) = x).</p>
</td></tr>
<tr valign="top"><td><code>use_bias</code></td>
<td>
<p>Whether the layer uses a bias vector.</p>
</td></tr>
<tr valign="top"><td><code>kernel_initializer</code></td>
<td>
<p>Initializer for the <code>kernel</code> weights matrix.</p>
</td></tr>
<tr valign="top"><td><code>bias_initializer</code></td>
<td>
<p>Initializer for the bias vector.</p>
</td></tr>
<tr valign="top"><td><code>kernel_regularizer</code></td>
<td>
<p>Regularizer function applied to the <code>kernel</code>
weights matrix.</p>
</td></tr>
<tr valign="top"><td><code>bias_regularizer</code></td>
<td>
<p>Regularizer function applied to the bias vector.</p>
</td></tr>
<tr valign="top"><td><code>activity_regularizer</code></td>
<td>
<p>Regularizer function applied to the output of the
layer (its &quot;activation&quot;)..</p>
</td></tr>
<tr valign="top"><td><code>kernel_constraint</code></td>
<td>
<p>Constraint function applied to the <code>kernel</code> weights
matrix.</p>
</td></tr>
<tr valign="top"><td><code>bias_constraint</code></td>
<td>
<p>Constraint function applied to the bias vector.</p>
</td></tr>
<tr valign="top"><td><code>input_shape</code></td>
<td>
<p>Dimensionality of the input (integer) not including the
samples axis. This argument is required when using this layer as the first
layer in a model.</p>
</td></tr>
<tr valign="top"><td><code>batch_input_shape</code></td>
<td>
<p>Shapes, including the batch size. For instance,
<code>batch_input_shape=c(10, 32)</code> indicates that the expected input will be
batches of 10 32-dimensional vectors. <code>batch_input_shape=list(NULL, 32)</code>
indicates batches of an arbitrary number of 32-dimensional vectors.</p>
</td></tr>
<tr valign="top"><td><code>batch_size</code></td>
<td>
<p>Fixed batch size for layer</p>
</td></tr>
<tr valign="top"><td><code>dtype</code></td>
<td>
<p>The data type expected by the input, as a string (<code>float32</code>,
<code>float64</code>, <code>int32</code>...)</p>
</td></tr>
<tr valign="top"><td><code>name</code></td>
<td>
<p>An optional name string for the layer. Should be unique in a
model (do not reuse the same name twice). It will be autogenerated if it
isn't provided.</p>
</td></tr>
<tr valign="top"><td><code>trainable</code></td>
<td>
<p>Whether the layer weights will be updated during training.</p>
</td></tr>
<tr valign="top"><td><code>weights</code></td>
<td>
<p>Initial weights for layer.</p>
</td></tr>
</table>


<h3>Input and Output Shapes</h3>

<p>Input shape: nD tensor with shape: <code style="white-space: pre;">(batch_size, ..., input_dim)</code>. The most
common situation would be a 2D input with shape <code style="white-space: pre;">(batch_size, input_dim)</code>.
</p>
<p>Output shape: nD tensor with shape: <code style="white-space: pre;">(batch_size, ..., units)</code>. For
instance, for a 2D input with shape <code style="white-space: pre;">(batch_size, input_dim)</code>, the output
would have shape <code style="white-space: pre;">(batch_size, unit)</code>.
</p>


<h3>See Also</h3>

<p>Other core layers: 
<code><a href="layer_activation.html">layer_activation</a>()</code>,
<code><a href="layer_activity_regularization.html">layer_activity_regularization</a>()</code>,
<code><a href="layer_attention.html">layer_attention</a>()</code>,
<code><a href="layer_dense_features.html">layer_dense_features</a>()</code>,
<code><a href="layer_dropout.html">layer_dropout</a>()</code>,
<code><a href="layer_flatten.html">layer_flatten</a>()</code>,
<code><a href="layer_input.html">layer_input</a>()</code>,
<code><a href="layer_lambda.html">layer_lambda</a>()</code>,
<code><a href="layer_masking.html">layer_masking</a>()</code>,
<code><a href="layer_permute.html">layer_permute</a>()</code>,
<code><a href="layer_repeat_vector.html">layer_repeat_vector</a>()</code>,
<code><a href="layer_reshape.html">layer_reshape</a>()</code>
</p>

<hr /><div style="text-align: center;">[Package <em>keras</em> version 2.6.1 <a href="00Index.html">Index</a>]</div>
</body></html>
