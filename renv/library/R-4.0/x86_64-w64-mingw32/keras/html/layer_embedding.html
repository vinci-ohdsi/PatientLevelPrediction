<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>R: Turns positive integers (indexes) into dense vectors of fixed...</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="R.css" />
</head><body>

<table width="100%" summary="page for layer_embedding {keras}"><tr><td>layer_embedding {keras}</td><td style="text-align: right;">R Documentation</td></tr></table>

<h2>Turns positive integers (indexes) into dense vectors of fixed size.</h2>

<h3>Description</h3>

<p>For example, <code>list(4L, 20L) -&gt; list(c(0.25, 0.1), c(0.6, -0.2))</code> This layer
can only be used as the first layer in a model.
</p>


<h3>Usage</h3>

<pre>
layer_embedding(
  object,
  input_dim,
  output_dim,
  embeddings_initializer = "uniform",
  embeddings_regularizer = NULL,
  activity_regularizer = NULL,
  embeddings_constraint = NULL,
  mask_zero = FALSE,
  input_length = NULL,
  batch_size = NULL,
  name = NULL,
  trainable = NULL,
  weights = NULL
)
</pre>


<h3>Arguments</h3>

<table summary="R argblock">
<tr valign="top"><td><code>object</code></td>
<td>
<p>What to call the new <code>Layer</code> instance with. Typically a keras
<code>Model</code>, another <code>Layer</code>, or a <code>tf.Tensor</code>/<code>KerasTensor</code>. If <code>object</code> is
missing, the <code>Layer</code> instance is returned, otherwise, <code>layer(object)</code> is
returned.</p>
</td></tr>
<tr valign="top"><td><code>input_dim</code></td>
<td>
<p>int &gt; 0. Size of the vocabulary, i.e. maximum integer
index + 1.</p>
</td></tr>
<tr valign="top"><td><code>output_dim</code></td>
<td>
<p>int &gt;= 0. Dimension of the dense embedding.</p>
</td></tr>
<tr valign="top"><td><code>embeddings_initializer</code></td>
<td>
<p>Initializer for the <code>embeddings</code> matrix.</p>
</td></tr>
<tr valign="top"><td><code>embeddings_regularizer</code></td>
<td>
<p>Regularizer function applied to the
<code>embeddings</code> matrix.</p>
</td></tr>
<tr valign="top"><td><code>activity_regularizer</code></td>
<td>
<p>activity_regularizer</p>
</td></tr>
<tr valign="top"><td><code>embeddings_constraint</code></td>
<td>
<p>Constraint function applied to the <code>embeddings</code>
matrix.</p>
</td></tr>
<tr valign="top"><td><code>mask_zero</code></td>
<td>
<p>Whether or not the input value 0 is a special &quot;padding&quot;
value that should be masked out. This is useful when using recurrent
layers, which may take variable length inputs. If this is <code>TRUE</code> then all
subsequent layers in the model need to support masking or an exception will
be raised. If mask_zero is set to TRUE, as a consequence, index 0 cannot be
used in the vocabulary (input_dim should equal size of vocabulary + 1).</p>
</td></tr>
<tr valign="top"><td><code>input_length</code></td>
<td>
<p>Length of input sequences, when it is constant. This
argument is required if you are going to connect <code>Flatten</code> then <code>Dense</code>
layers upstream (without it, the shape of the dense outputs cannot be
computed).</p>
</td></tr>
<tr valign="top"><td><code>batch_size</code></td>
<td>
<p>Fixed batch size for layer</p>
</td></tr>
<tr valign="top"><td><code>name</code></td>
<td>
<p>An optional name string for the layer. Should be unique in a
model (do not reuse the same name twice). It will be autogenerated if it
isn't provided.</p>
</td></tr>
<tr valign="top"><td><code>trainable</code></td>
<td>
<p>Whether the layer weights will be updated during training.</p>
</td></tr>
<tr valign="top"><td><code>weights</code></td>
<td>
<p>Initial weights for layer.</p>
</td></tr>
</table>


<h3>Input shape</h3>

<p>2D tensor with shape: <code style="white-space: pre;">(batch_size, sequence_length)</code>.
</p>


<h3>Output shape</h3>

<p>3D tensor with shape: <code style="white-space: pre;">(batch_size, sequence_length, output_dim)</code>.
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1512.05287">A Theoretically Grounded Application of Dropout in Recurrent Neural Networks</a>
</p>
</li></ul>


<hr /><div style="text-align: center;">[Package <em>keras</em> version 2.6.1 <a href="00Index.html">Index</a>]</div>
</body></html>
