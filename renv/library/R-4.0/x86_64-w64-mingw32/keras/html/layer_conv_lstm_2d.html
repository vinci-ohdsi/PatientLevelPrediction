<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>R: Convolutional LSTM.</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="R.css" />
</head><body>

<table width="100%" summary="page for layer_conv_lstm_2d {keras}"><tr><td>layer_conv_lstm_2d {keras}</td><td style="text-align: right;">R Documentation</td></tr></table>

<h2>Convolutional LSTM.</h2>

<h3>Description</h3>

<p>It is similar to an LSTM layer, but the input transformations and recurrent
transformations are both convolutional.
</p>


<h3>Usage</h3>

<pre>
layer_conv_lstm_2d(
  object,
  filters,
  kernel_size,
  strides = c(1L, 1L),
  padding = "valid",
  data_format = NULL,
  dilation_rate = c(1L, 1L),
  activation = "tanh",
  recurrent_activation = "hard_sigmoid",
  use_bias = TRUE,
  kernel_initializer = "glorot_uniform",
  recurrent_initializer = "orthogonal",
  bias_initializer = "zeros",
  unit_forget_bias = TRUE,
  kernel_regularizer = NULL,
  recurrent_regularizer = NULL,
  bias_regularizer = NULL,
  activity_regularizer = NULL,
  kernel_constraint = NULL,
  recurrent_constraint = NULL,
  bias_constraint = NULL,
  return_sequences = FALSE,
  return_state = FALSE,
  go_backwards = FALSE,
  stateful = FALSE,
  dropout = 0,
  recurrent_dropout = 0,
  batch_size = NULL,
  name = NULL,
  trainable = NULL,
  weights = NULL,
  input_shape = NULL
)
</pre>


<h3>Arguments</h3>

<table summary="R argblock">
<tr valign="top"><td><code>object</code></td>
<td>
<p>What to call the new <code>Layer</code> instance with. Typically a keras
<code>Model</code>, another <code>Layer</code>, or a <code>tf.Tensor</code>/<code>KerasTensor</code>. If <code>object</code> is
missing, the <code>Layer</code> instance is returned, otherwise, <code>layer(object)</code> is
returned.</p>
</td></tr>
<tr valign="top"><td><code>filters</code></td>
<td>
<p>Integer, the dimensionality of the output space (i.e. the
number of output filters in the convolution).</p>
</td></tr>
<tr valign="top"><td><code>kernel_size</code></td>
<td>
<p>An integer or list of n integers, specifying the
dimensions of the convolution window.</p>
</td></tr>
<tr valign="top"><td><code>strides</code></td>
<td>
<p>An integer or list of n integers, specifying the strides of
the convolution. Specifying any stride value != 1 is incompatible with
specifying any <code>dilation_rate</code> value != 1.</p>
</td></tr>
<tr valign="top"><td><code>padding</code></td>
<td>
<p>One of <code>"valid"</code> or <code>"same"</code> (case-insensitive).</p>
</td></tr>
<tr valign="top"><td><code>data_format</code></td>
<td>
<p>A string, one of <code>channels_last</code> (default) or
<code>channels_first</code>. The ordering of the dimensions in the inputs.
<code>channels_last</code> corresponds to inputs with shape <code style="white-space: pre;">(batch, time, ..., channels)</code> while <code>channels_first</code> corresponds to inputs with shape <code style="white-space: pre;">(batch, time, channels, ...)</code>. It defaults to the <code>image_data_format</code> value found
in your Keras config file at <code style="white-space: pre;">~/.keras/keras.json</code>. If you never set it,
then it will be &quot;channels_last&quot;.</p>
</td></tr>
<tr valign="top"><td><code>dilation_rate</code></td>
<td>
<p>An integer or list of n integers, specifying the
dilation rate to use for dilated convolution. Currently, specifying any
<code>dilation_rate</code> value != 1 is incompatible with specifying any <code>strides</code>
value != 1.</p>
</td></tr>
<tr valign="top"><td><code>activation</code></td>
<td>
<p>Activation function to use. If you don't specify anything,
no activation is applied (ie. &quot;linear&quot; activation: <code>a(x) = x</code>).</p>
</td></tr>
<tr valign="top"><td><code>recurrent_activation</code></td>
<td>
<p>Activation function to use for the recurrent
step.</p>
</td></tr>
<tr valign="top"><td><code>use_bias</code></td>
<td>
<p>Boolean, whether the layer uses a bias vector.</p>
</td></tr>
<tr valign="top"><td><code>kernel_initializer</code></td>
<td>
<p>Initializer for the <code>kernel</code> weights matrix, used
for the linear transformation of the inputs..</p>
</td></tr>
<tr valign="top"><td><code>recurrent_initializer</code></td>
<td>
<p>Initializer for the <code>recurrent_kernel</code> weights
matrix, used for the linear transformation of the recurrent state..</p>
</td></tr>
<tr valign="top"><td><code>bias_initializer</code></td>
<td>
<p>Initializer for the bias vector.</p>
</td></tr>
<tr valign="top"><td><code>unit_forget_bias</code></td>
<td>
<p>Boolean. If TRUE, add 1 to the bias of the forget
gate at initialization. Use in combination with <code>bias_initializer="zeros"</code>.
This is recommended in <a href="https://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf">Jozefowicz et al.</a></p>
</td></tr>
<tr valign="top"><td><code>kernel_regularizer</code></td>
<td>
<p>Regularizer function applied to the <code>kernel</code>
weights matrix.</p>
</td></tr>
<tr valign="top"><td><code>recurrent_regularizer</code></td>
<td>
<p>Regularizer function applied to the
<code>recurrent_kernel</code> weights matrix.</p>
</td></tr>
<tr valign="top"><td><code>bias_regularizer</code></td>
<td>
<p>Regularizer function applied to the bias vector.</p>
</td></tr>
<tr valign="top"><td><code>activity_regularizer</code></td>
<td>
<p>Regularizer function applied to the output of the
layer (its &quot;activation&quot;)..</p>
</td></tr>
<tr valign="top"><td><code>kernel_constraint</code></td>
<td>
<p>Constraint function applied to the <code>kernel</code> weights
matrix.</p>
</td></tr>
<tr valign="top"><td><code>recurrent_constraint</code></td>
<td>
<p>Constraint function applied to the
<code>recurrent_kernel</code> weights matrix.</p>
</td></tr>
<tr valign="top"><td><code>bias_constraint</code></td>
<td>
<p>Constraint function applied to the bias vector.</p>
</td></tr>
<tr valign="top"><td><code>return_sequences</code></td>
<td>
<p>Boolean. Whether to return the last output in the
output sequence, or the full sequence.</p>
</td></tr>
<tr valign="top"><td><code>return_state</code></td>
<td>
<p>Boolean. Whether to return the last state in addition to the output.</p>
</td></tr>
<tr valign="top"><td><code>go_backwards</code></td>
<td>
<p>Boolean (default FALSE). If TRUE, rocess the input
sequence backwards.</p>
</td></tr>
<tr valign="top"><td><code>stateful</code></td>
<td>
<p>Boolean (default FALSE). If TRUE, the last state for each
sample at index i in a batch will be used as initial state for the sample
of index i in the following batch.</p>
</td></tr>
<tr valign="top"><td><code>dropout</code></td>
<td>
<p>Float between 0 and 1. Fraction of the units to drop for the
linear transformation of the inputs.</p>
</td></tr>
<tr valign="top"><td><code>recurrent_dropout</code></td>
<td>
<p>Float between 0 and 1. Fraction of the units to drop
for the linear transformation of the recurrent state.</p>
</td></tr>
<tr valign="top"><td><code>batch_size</code></td>
<td>
<p>Fixed batch size for layer</p>
</td></tr>
<tr valign="top"><td><code>name</code></td>
<td>
<p>An optional name string for the layer. Should be unique in a
model (do not reuse the same name twice). It will be autogenerated if it
isn't provided.</p>
</td></tr>
<tr valign="top"><td><code>trainable</code></td>
<td>
<p>Whether the layer weights will be updated during training.</p>
</td></tr>
<tr valign="top"><td><code>weights</code></td>
<td>
<p>Initial weights for layer.</p>
</td></tr>
<tr valign="top"><td><code>input_shape</code></td>
<td>
<p>Dimensionality of the input (integer) not including the
samples axis. This argument is required when using this layer as the first
layer in a model.</p>
</td></tr>
</table>


<h3>Input shape</h3>


<ul>
<li><p> if data_format='channels_first' 5D tensor with shape:
<code style="white-space: pre;">(samples,time, channels, rows, cols)</code>
</p>

<ul>
<li><p> if data_format='channels_last' 5D
tensor with shape: <code style="white-space: pre;">(samples,time, rows, cols, channels)</code>
</p>
</li></ul>

</li></ul>



<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1506.04214v1">Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting</a>
The current implementation does not include the feedback loop on the cells
output
</p>
</li></ul>



<h3>See Also</h3>

<p>Other convolutional layers: 
<code><a href="layer_conv_1d_transpose.html">layer_conv_1d_transpose</a>()</code>,
<code><a href="layer_conv_1d.html">layer_conv_1d</a>()</code>,
<code><a href="layer_conv_2d_transpose.html">layer_conv_2d_transpose</a>()</code>,
<code><a href="layer_conv_2d.html">layer_conv_2d</a>()</code>,
<code><a href="layer_conv_3d_transpose.html">layer_conv_3d_transpose</a>()</code>,
<code><a href="layer_conv_3d.html">layer_conv_3d</a>()</code>,
<code><a href="layer_cropping_1d.html">layer_cropping_1d</a>()</code>,
<code><a href="layer_cropping_2d.html">layer_cropping_2d</a>()</code>,
<code><a href="layer_cropping_3d.html">layer_cropping_3d</a>()</code>,
<code><a href="layer_depthwise_conv_2d.html">layer_depthwise_conv_2d</a>()</code>,
<code><a href="layer_separable_conv_1d.html">layer_separable_conv_1d</a>()</code>,
<code><a href="layer_separable_conv_2d.html">layer_separable_conv_2d</a>()</code>,
<code><a href="layer_upsampling_1d.html">layer_upsampling_1d</a>()</code>,
<code><a href="layer_upsampling_2d.html">layer_upsampling_2d</a>()</code>,
<code><a href="layer_upsampling_3d.html">layer_upsampling_3d</a>()</code>,
<code><a href="layer_zero_padding_1d.html">layer_zero_padding_1d</a>()</code>,
<code><a href="layer_zero_padding_2d.html">layer_zero_padding_2d</a>()</code>,
<code><a href="layer_zero_padding_3d.html">layer_zero_padding_3d</a>()</code>
</p>

<hr /><div style="text-align: center;">[Package <em>keras</em> version 2.6.1 <a href="00Index.html">Index</a>]</div>
</body></html>
